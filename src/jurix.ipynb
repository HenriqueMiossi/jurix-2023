{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import Tuple, List\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Filesystem:\n",
    "    def is_file_valid_json(self, path: str) -> bool:\n",
    "        if not os.path.isfile(path):\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            f = open(path, encoding=\"utf8\")\n",
    "            json.load(f)\n",
    "            return True\n",
    "        except ValueError:  # includes JSONDecodeError\n",
    "            return False\n",
    "\n",
    "    def get_dataset(self, dataset_size: int) -> Tuple[List[str], List[str]]:\n",
    "        dir = \"../dataset\"\n",
    "        test_dir = \"../dataset-test/\"\n",
    "        train_dataset = []\n",
    "        test_dataset = []\n",
    "\n",
    "        # Clean macOS specific files\n",
    "        workdir = os.listdir(dir)\n",
    "        test_workdir = os.listdir(test_dir)\n",
    "        if \".DS_Store\" in workdir:\n",
    "            workdir.remove(\".DS_Store\")\n",
    "\n",
    "        if \".DS_Store\" in test_workdir:\n",
    "            workdir.remove(\".DS_Store\")\n",
    "\n",
    "        # Always randomize the training dataset\n",
    "        random.shuffle(workdir)\n",
    "        enumeration = enumerate(workdir)\n",
    "\n",
    "        for i, filename in enumeration:\n",
    "            path = os.path.join(dir, filename)\n",
    "            # checking if it is a file\n",
    "            if self.is_file_valid_json(path) and i < dataset_size:\n",
    "                f = open(path, encoding=\"utf8\")\n",
    "                data = json.load(f)\n",
    "\n",
    "                entities = []\n",
    "\n",
    "                for item in data[\"items\"]:\n",
    "                    # if item[\"type\"] != \"RATIO_DECIDENDI\" and item[\"type\"] != \"SUBJECT\":\n",
    "                    entities.append((item[\"start\"], item[\"end\"], item[\"type\"]))\n",
    "\n",
    "                train_dataset.append([data[\"source\"], {\"entities\": entities}])\n",
    "\n",
    "            # elif self.is_file_valid_json(path) and i >= dataset_size:\n",
    "            #     f = open(path, encoding=\"utf8\")\n",
    "            #     data = json.load(f)\n",
    "\n",
    "            #     entities = []\n",
    "\n",
    "            #     for item in data[\"items\"]:\n",
    "            #         # if (item[\"type\"] != \"RATIO_DECIDENDI\" and item[\"type\"] != \"SUBJECT\"):\n",
    "            #         entities.append(\n",
    "            #             (\n",
    "            #                 item[\"start\"],\n",
    "            #                 item[\"end\"],\n",
    "            #                 item[\"type\"],\n",
    "            #                 item[\"selected-text\"],\n",
    "            #             )\n",
    "            #         )\n",
    "\n",
    "            #     test_dataset.append([data[\"source\"], {\"entities\": entities}])\n",
    "\n",
    "        for filename in test_workdir:\n",
    "            path = os.path.join(test_dir, filename)\n",
    "            # checking if it is a file\n",
    "            if self.is_file_valid_json(path):\n",
    "                f = open(path, encoding=\"utf8\")\n",
    "                data = json.load(f)\n",
    "\n",
    "                entities = []\n",
    "\n",
    "                for item in data[\"items\"]:\n",
    "                    # if item[\"type\"] != \"RATIO_DECIDENDI\" and item[\"type\"] != \"SUBJECT\":\n",
    "                    entities.append(\n",
    "                        (\n",
    "                            item[\"start\"],\n",
    "                            item[\"end\"],\n",
    "                            item[\"type\"],\n",
    "                            item[\"selected-text\"],\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                test_dataset.append([data[\"source\"], {\"entities\": entities}])\n",
    "\n",
    "        return (train_dataset, test_dataset)\n",
    "\n",
    "    def get_models(self):\n",
    "        return [\n",
    "            \"../models/\" + d\n",
    "            for d in os.listdir(\"../models\")\n",
    "            if os.path.isdir(os.path.join(\"../models\", d))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def run(self):\n",
    "        # Get a randomized TRAIN_DATA with specified size\n",
    "        dataset_sizes_str = input(\"Insert the dataset size for training: \").split()\n",
    "        dataset_sizes_int = list(set([int(item) for item in dataset_sizes_str]))\n",
    "\n",
    "        for dataset_size in dataset_sizes_int:\n",
    "            dataset = fs.get_dataset(dataset_size)\n",
    "            TRAIN_DATA = dataset[0]\n",
    "            TEST_DATA = dataset[1]\n",
    "\n",
    "            n_iter = 10\n",
    "            random.seed(0)\n",
    "\n",
    "            # Create blank model\n",
    "            nlp = spacy.blank(\"pt\")\n",
    "            # nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "            ner = None\n",
    "\n",
    "            # Get ner pipeline component (create if necessary)\n",
    "            if \"ner\" not in nlp.pipe_names:\n",
    "                ner = nlp.create_pipe(\"ner\")\n",
    "                nlp.add_pipe(ner)\n",
    "            else:\n",
    "                ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "            # Add new entity labels to entity recognizer\n",
    "            labels = []\n",
    "            for _, entities in TRAIN_DATA:\n",
    "                e = entities[\"entities\"]\n",
    "                [labels.append(entity[2]) for entity in e]\n",
    "            labels = set(labels)\n",
    "            [ner.add_label(l) for l in labels]\n",
    "\n",
    "            # Set optimizer\n",
    "            optimizer = nlp.begin_training()\n",
    "            # optimizer = nlp.resume_training()\n",
    "\n",
    "            # Get names of other pipes to disable them during training\n",
    "            other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "\n",
    "            # Only train NER pipe\n",
    "            with nlp.disable_pipes(*other_pipes):\n",
    "                # Process our training examples in iterations using shuffle, batches, and dropouts\n",
    "                sizes = compounding(1, 16, 1.001)\n",
    "                for itn in range(n_iter):\n",
    "                    random.shuffle(TRAIN_DATA)\n",
    "\n",
    "                    batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "                    losses = {}\n",
    "                    misses = 0\n",
    "                    total_items = 0\n",
    "                    for batch in batches:\n",
    "                        texts, annotations = zip(*batch)\n",
    "\n",
    "                        # For each example, nlp.update steps through the words of the input\n",
    "                        # At each word, it makes a prediction on the text and checks the annotations\n",
    "                        # If it was wrong, it adjusts its weights\n",
    "                        try:\n",
    "                            nlp.update(\n",
    "                                texts,\n",
    "                                annotations,\n",
    "                                sgd=optimizer,\n",
    "                                drop=0.2,\n",
    "                                losses=losses,\n",
    "                            )\n",
    "                            total_items += 1\n",
    "                        except Exception as e:\n",
    "                            misses += 1\n",
    "                            # print(f\"item , error: {e}\")\n",
    "                    # print(\"Losses\", losses)\n",
    "                    # print(\"Misses\", misses)\n",
    "                    # print(\"Total Items\", total_items)\n",
    "\n",
    "            # Save model to output directory\n",
    "            nlp.meta[\"name\"] = f\"juridic-{dataset_size}\"\n",
    "            nlp.to_disk(f\"../models/juridic-{dataset_size}\")\n",
    "\n",
    "            mt.run(f\"../models/juridic-{dataset_size}\", TEST_DATA)\n",
    "            mtt.run(f\"../models/juridic-{dataset_size}\", TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def find_matching_item(self, givenItem, givenJson):\n",
    "        # alias\n",
    "        selected_text = 3\n",
    "\n",
    "        for jsonItem in givenJson[\"entities\"]:\n",
    "            if givenItem[selected_text] == jsonItem[selected_text]:\n",
    "                return jsonItem\n",
    "        return None\n",
    "\n",
    "    def find_not_matching_items(\n",
    "        self, expectedItems, fullItems, outputItems\n",
    "    ):  # expected fullJson output\n",
    "        # Aliases\n",
    "        selected_text = 3\n",
    "\n",
    "        missing_items = []\n",
    "\n",
    "        for fullItem in fullItems[\"entities\"]:\n",
    "            found = False\n",
    "            for expectedItem in expectedItems[\"entities\"]:\n",
    "                if (\n",
    "                    expectedItem[selected_text] == fullItem[0]\n",
    "                ):  # fullItem selected_text is misaligned\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                missing_items.append(fullItem)\n",
    "\n",
    "        count_not_found_in_output = 0\n",
    "        for missingItem in missing_items:\n",
    "            for outputItem in outputItems[\"entities\"]:\n",
    "                if missingItem[selected_text] != outputItem[selected_text]:\n",
    "                    count_not_found_in_output += 1\n",
    "                    break\n",
    "\n",
    "        return count_not_found_in_output\n",
    "\n",
    "    def create_json_all_items(self, expectedJson):\n",
    "        # aliases\n",
    "        source = 0\n",
    "        items = 1\n",
    "\n",
    "        all_items = []\n",
    "\n",
    "        all_itemsJson = [expectedJson[source]]\n",
    "\n",
    "        for item in expectedJson[items][\"entities\"]:\n",
    "            current_items = (\n",
    "                item[3],  # Text\n",
    "                item[0],  # Start\n",
    "                item[1],  # End\n",
    "                item[2],  # Tag / Type\n",
    "            )\n",
    "            all_items.append(current_items)\n",
    "\n",
    "        current_start = 0\n",
    "        for item in expectedJson[items][\"entities\"]:\n",
    "            current_end = item[0]  # Start\n",
    "            # if current_end == 0: continue\n",
    "\n",
    "            text_not_used_item = expectedJson[source][current_start:current_end].strip()\n",
    "\n",
    "            if text_not_used_item and text_not_used_item.strip() != \".\":\n",
    "                not_used_item = (\n",
    "                    text_not_used_item,\n",
    "                    current_start,\n",
    "                    current_end,\n",
    "                    \"NOT_USED\",\n",
    "                )\n",
    "                all_items.append(not_used_item)\n",
    "\n",
    "            current_start = item[1]  # End\n",
    "\n",
    "        if current_start < len(expectedJson[source]):\n",
    "            final_text = expectedJson[source][current_start:].strip()\n",
    "\n",
    "            if final_text and final_text.strip() != \".\":\n",
    "                not_used_item = (\n",
    "                    final_text,\n",
    "                    current_start,\n",
    "                    len(expectedJson[source]),  # End\n",
    "                    \"NOT_USED\",\n",
    "                )\n",
    "                all_items.append(not_used_item)\n",
    "\n",
    "        all_itemsJson.append({\"entities\": all_items})\n",
    "\n",
    "        return all_itemsJson\n",
    "\n",
    "    def calculate_accuracy(self, expectedJson, outputJson):\n",
    "        # aliases\n",
    "        items = 1\n",
    "        selected_text = 3\n",
    "        type = 2\n",
    "\n",
    "        fullJson = self.create_json_all_items(outputJson)\n",
    "\n",
    "        expectedTotalItems = len(expectedJson[items][\"entities\"])\n",
    "        outputTotalItems = len(outputJson[items][\"entities\"])\n",
    "\n",
    "        incorrectMatches = 0\n",
    "        correctMatches = 0\n",
    "        itemsFound = 0\n",
    "\n",
    "        for item in expectedJson[items][\"entities\"]:\n",
    "            matchingItem = self.find_matching_item(item, outputJson[items])\n",
    "            if matchingItem and item[selected_text] == matchingItem[selected_text]:\n",
    "                correctMatches += 1\n",
    "                itemsFound += 1\n",
    "            if matchingItem and item[type] == matchingItem[type]:\n",
    "                correctMatches += 1\n",
    "            if matchingItem and item[type] != matchingItem[type]:\n",
    "                incorrectMatches += 1\n",
    "\n",
    "        trueNegatives = (\n",
    "            self.find_not_matching_items(\n",
    "                expectedJson[items], fullJson[items], outputJson[items]\n",
    "            )\n",
    "            * 2\n",
    "        )\n",
    "        truePositives = correctMatches\n",
    "        falsePositives = ((outputTotalItems - itemsFound) * 2) + incorrectMatches\n",
    "        falseNegatives = (expectedTotalItems - itemsFound) * 2\n",
    "\n",
    "        return truePositives, falsePositives, trueNegatives, falseNegatives\n",
    "\n",
    "    def run(self, model, test_data):\n",
    "        nlp = spacy.load(model)\n",
    "\n",
    "        expected_list = test_data\n",
    "\n",
    "        inferred_list = [nlp(item[0]) for item in test_data]\n",
    "        given_list = []\n",
    "\n",
    "        for item in inferred_list:\n",
    "            given_list.append(\n",
    "                [\n",
    "                    item.text,\n",
    "                    {\n",
    "                        \"entities\": [\n",
    "                            (ent.start_char, ent.end_char, ent.label_, ent.text)\n",
    "                            for ent in item.ents\n",
    "                        ]\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        totalTruePositives = 0\n",
    "        totalFalsePositives = 0\n",
    "        totalTrueNegatives = 0\n",
    "        totalFalseNegatives = 0\n",
    "\n",
    "        for i in range(len(expected_list)):\n",
    "            (\n",
    "                truePositives,\n",
    "                falsePositives,\n",
    "                trueNegatives,\n",
    "                falseNegatives,\n",
    "            ) = self.calculate_accuracy(expected_list[i], given_list[i])\n",
    "\n",
    "            totalTruePositives += truePositives\n",
    "            totalFalsePositives += falsePositives\n",
    "            totalTrueNegatives += trueNegatives\n",
    "            totalFalseNegatives += falseNegatives\n",
    "\n",
    "        try:\n",
    "            accuracy = (truePositives + trueNegatives) / (\n",
    "                truePositives + falsePositives + trueNegatives + falseNegatives\n",
    "            )\n",
    "        except ZeroDivisionError:\n",
    "            accuracy = 0\n",
    "\n",
    "        try:\n",
    "            precision = truePositives / (truePositives + falsePositives)\n",
    "        except ZeroDivisionError:\n",
    "            precision = 0\n",
    "\n",
    "        try:\n",
    "            recall = truePositives / (truePositives + falseNegatives)\n",
    "        except ZeroDivisionError:\n",
    "            recall = 0\n",
    "\n",
    "        try:\n",
    "            f1score = 2 * (precision * recall) / (precision + recall)\n",
    "        except ZeroDivisionError:\n",
    "            f1score = 0\n",
    "\n",
    "        model_metrics.loc[int(re.sub(\"[^\\d]\", \"\", nlp.meta[\"name\"]))] = [\n",
    "            nlp.meta[\"name\"],  # Model name\n",
    "            accuracy,\n",
    "            precision,\n",
    "            recall,\n",
    "            f1score,\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagMetric:\n",
    "    def __init__(self, tag: str) -> None:\n",
    "        self.tag = tag\n",
    "        self.foundItems = 0\n",
    "        self.correctMatches = 0\n",
    "        self.incorrectMatches = 0\n",
    "\n",
    "        self.truePositives = 0\n",
    "        self.falsePositives = 0\n",
    "        self.trueNegatives = 0\n",
    "        self.falseNegatives = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsByTag:\n",
    "    def find_matching_item(self, givenItem, givenJson):\n",
    "        # alias\n",
    "        selected_text = 3\n",
    "\n",
    "        for jsonItem in givenJson[\"entities\"]:\n",
    "            if givenItem[selected_text] == jsonItem[selected_text]:\n",
    "                return jsonItem\n",
    "        return None\n",
    "\n",
    "    def find_not_matching_items(\n",
    "        self, expectedItems, fullItems, outputItems, tag\n",
    "    ):  # expected fullJson output\n",
    "        # Aliases\n",
    "        selected_text = 3\n",
    "        type = 2\n",
    "\n",
    "        missing_items = []\n",
    "\n",
    "        for fullItem in fullItems[\"entities\"]:\n",
    "            found = False\n",
    "            for expectedItem in expectedItems[\"entities\"]:\n",
    "                if (\n",
    "                    expectedItem[selected_text] == fullItem[0]\n",
    "                    and expectedItem[type] == tag\n",
    "                ):  # fullItem selected_text is misaligned\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                missing_items.append(fullItem)\n",
    "\n",
    "        count_not_found_in_output = 0\n",
    "        for missingItem in missing_items:\n",
    "            for outputItem in outputItems[\"entities\"]:\n",
    "                if (\n",
    "                    missingItem[selected_text] != outputItem[selected_text]\n",
    "                    and missingItem[type] == tag\n",
    "                ):\n",
    "                    count_not_found_in_output += 1\n",
    "                    break\n",
    "\n",
    "        return count_not_found_in_output\n",
    "\n",
    "    def create_json_all_items(self, expectedJson):\n",
    "        # aliases\n",
    "        source = 0\n",
    "        items = 1\n",
    "\n",
    "        all_items = []\n",
    "\n",
    "        all_itemsJson = [expectedJson[source]]\n",
    "\n",
    "        for item in expectedJson[items][\"entities\"]:\n",
    "            current_items = (\n",
    "                item[3],  # Text\n",
    "                item[0],  # Start\n",
    "                item[1],  # End\n",
    "                item[2],  # Tag / Type\n",
    "            )\n",
    "            all_items.append(current_items)\n",
    "\n",
    "        current_start = 0\n",
    "        for item in expectedJson[items][\"entities\"]:\n",
    "            current_end = item[0]  # Start\n",
    "            # if current_end == 0: continue\n",
    "\n",
    "            text_not_used_item = expectedJson[source][current_start:current_end].strip()\n",
    "\n",
    "            if text_not_used_item and text_not_used_item.strip() != \".\":\n",
    "                not_used_item = (\n",
    "                    text_not_used_item,\n",
    "                    current_start,\n",
    "                    current_end,\n",
    "                    \"NOT_USED\",\n",
    "                )\n",
    "                all_items.append(not_used_item)\n",
    "\n",
    "            current_start = item[1]  # End\n",
    "\n",
    "        if current_start < len(expectedJson[source]):\n",
    "            final_text = expectedJson[source][current_start:].strip()\n",
    "\n",
    "            if final_text and final_text.strip() != \".\":\n",
    "                not_used_item = (\n",
    "                    final_text,\n",
    "                    current_start,\n",
    "                    len(expectedJson[source]),  # End\n",
    "                    \"NOT_USED\",\n",
    "                )\n",
    "                all_items.append(not_used_item)\n",
    "\n",
    "        all_itemsJson.append({\"entities\": all_items})\n",
    "\n",
    "        return all_itemsJson\n",
    "\n",
    "    def calculate_accuracy(self, expectedJson, outputJson):\n",
    "        # aliases\n",
    "        items = 1\n",
    "        selected_text = 3\n",
    "        type = 2\n",
    "\n",
    "        fullJson = self.create_json_all_items(outputJson)\n",
    "\n",
    "        tagMetrics = []\n",
    "\n",
    "        for item in expectedJson[items][\"entities\"]:\n",
    "            matchingItem = self.find_matching_item(item, outputJson[items])\n",
    "            if matchingItem and item[selected_text] == matchingItem[selected_text]:\n",
    "                tagMetric = next((x for x in tagMetrics if x.tag == item[type]), None)\n",
    "                if tagMetric != None:\n",
    "                    tagMetric.correctMatches += 1\n",
    "                    tagMetric.foundItems += 1\n",
    "                else:\n",
    "                    newTagMetric = TagMetric(item[type])\n",
    "                    newTagMetric.correctMatches += 1\n",
    "                    newTagMetric.foundItems += 1\n",
    "                    tagMetrics.append(newTagMetric)\n",
    "\n",
    "            if matchingItem and item[type] == matchingItem[type]:\n",
    "                tagMetric = next((x for x in tagMetrics if x.tag == item[type]), None)\n",
    "                if tagMetric != None:\n",
    "                    tagMetric.correctMatches += 1\n",
    "                else:\n",
    "                    newTagMetric = TagMetric(item[type])\n",
    "                    newTagMetric.correctMatches += 1\n",
    "                    tagMetrics.append(newTagMetric)\n",
    "\n",
    "            if matchingItem and item[type] != matchingItem[type]:\n",
    "                tagMetric = next((x for x in tagMetrics if x.tag == item[type]), None)\n",
    "                if tagMetric != None:\n",
    "                    tagMetric.incorrectMatches += 1\n",
    "                else:\n",
    "                    newTagMetric = TagMetric(item[type])\n",
    "                    newTagMetric.incorrectMatches += 1\n",
    "                    tagMetrics.append(newTagMetric)\n",
    "\n",
    "        for tagMetric in tagMetrics:\n",
    "            expectedTotalItems = sum(\n",
    "                i[type] == tagMetric.tag for i in expectedJson[items][\"entities\"]\n",
    "            )\n",
    "            outputTotalItems = sum(\n",
    "                i[type] == tagMetric.tag for i in outputJson[items][\"entities\"]\n",
    "            )\n",
    "\n",
    "            tagMetric.trueNegatives = (\n",
    "                self.find_not_matching_items(\n",
    "                    expectedJson[items],\n",
    "                    fullJson[items],\n",
    "                    outputJson[items],\n",
    "                    tagMetric.tag,\n",
    "                )\n",
    "                * 2\n",
    "            )\n",
    "\n",
    "            tagMetric.truePositives = tagMetric.correctMatches\n",
    "            tagMetric.falsePositives = abs(\n",
    "                ((outputTotalItems - tagMetric.foundItems) * 2)\n",
    "                + tagMetric.incorrectMatches\n",
    "            )\n",
    "            tagMetric.falseNegatives = abs(\n",
    "                (expectedTotalItems - tagMetric.foundItems) * 2\n",
    "            )\n",
    "\n",
    "        # truePositives = correctMatches\n",
    "        # falsePositives = ((outputTotalItems - itemsFound) * 2) + incorrectMatches\n",
    "        # falseNegatives = (expectedTotalItems - itemsFound) * 2\n",
    "        return tagMetrics\n",
    "\n",
    "    def run(self, model, test_data):\n",
    "        nlp = spacy.load(model)\n",
    "\n",
    "        expected_list = test_data\n",
    "\n",
    "        inferred_list = [nlp(item[0]) for item in test_data]\n",
    "        given_list = []\n",
    "\n",
    "        for item in inferred_list:\n",
    "            given_list.append(\n",
    "                [\n",
    "                    item.text,\n",
    "                    {\n",
    "                        \"entities\": [\n",
    "                            (ent.start_char, ent.end_char, ent.label_, ent.text)\n",
    "                            for ent in item.ents\n",
    "                        ]\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        tagMetrics = []\n",
    "\n",
    "        for i in range(len(expected_list)):\n",
    "            newItems = self.calculate_accuracy(expected_list[i], given_list[i])\n",
    "            for item in newItems:\n",
    "                tagMetric = next((x for x in tagMetrics if x.tag == item.tag), None)\n",
    "                if tagMetric != None:\n",
    "                    tagMetric.truePositives = item.truePositives\n",
    "                    tagMetric.falsePositives = item.falsePositives\n",
    "                    tagMetric.trueNegatives = item.trueNegatives\n",
    "                    tagMetric.falseNegatives = item.falseNegatives\n",
    "                else:\n",
    "                    tagMetrics.append(item)\n",
    "\n",
    "        for tag in tagMetrics:\n",
    "            accuracy = 0\n",
    "            precision = 0\n",
    "            recall = 0\n",
    "            f1score = 0\n",
    "\n",
    "            try:\n",
    "                accuracy = (tag.truePositives + tag.trueNegatives) / (\n",
    "                    tag.truePositives\n",
    "                    + tag.falsePositives\n",
    "                    + tag.trueNegatives\n",
    "                    + tag.falseNegatives\n",
    "                )\n",
    "            except ZeroDivisionError:\n",
    "                accuracy = 0\n",
    "\n",
    "            try:\n",
    "                precision = tag.truePositives / (tag.truePositives + tag.falsePositives)\n",
    "            except ZeroDivisionError:\n",
    "                precision = 0\n",
    "\n",
    "            try:\n",
    "                recall = tag.truePositives / (tag.truePositives + tag.falseNegatives)\n",
    "            except ZeroDivisionError:\n",
    "                recall = 0\n",
    "\n",
    "            try:\n",
    "                f1score = 2 * (precision * recall) / (precision + recall)\n",
    "            except ZeroDivisionError:\n",
    "                f1score = 0\n",
    "\n",
    "            tag_metrics.loc[(int(re.sub(\"[^\\d]\", \"\", nlp.meta[\"name\"])), tag.tag),:] = [\n",
    "                nlp.meta[\"name\"],  # Model name\n",
    "                accuracy,\n",
    "                precision,\n",
    "                recall,\n",
    "                f1score,\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = Filesystem()\n",
    "tr = Training()\n",
    "mt = Metrics()\n",
    "mtt = MetricsByTag()\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "model_metrics = pd.DataFrame(\n",
    "    columns=[\"model_name\", \"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n",
    ")\n",
    "\n",
    "tag_metrics = pd.DataFrame(\n",
    "    columns = [\"model_name\", \"accuracy\", \"precision\", \"recall\", \"f1_score\"],\n",
    "    index = pd.MultiIndex(levels=[[], []], codes=[[], []], names=[\"dataset_size\", \"tag\"])\n",
    ")\n",
    "\n",
    "tr.run()\n",
    "\n",
    "model_metrics.sort_index(inplace=True)\n",
    "tag_metrics.sort_index(inplace=True)\n",
    "\n",
    "print(\"Performance evolution across different dataset sizes\")\n",
    "display(model_metrics)\n",
    "\n",
    "model_metrics[[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]].plot()\n",
    "plt.title(\"Metrics for Each Model\")\n",
    "plt.xlabel(\"Dataset Size\")\n",
    "plt.xticks(model_metrics.index)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.show()\n",
    "\n",
    "print(model_metrics.to_latex())\n",
    "\n",
    "print(\"Performance for each tag with the largest dataset model\")\n",
    "best_model_tag_metrics = tag_metrics[[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]].loc[tag_metrics.index.get_level_values(0).max()]\n",
    "\n",
    "display(best_model_tag_metrics)\n",
    "\n",
    "best_model_tag_metrics[[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]].plot.bar()\n",
    "plt.title(\"Metrics for Each Tag\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()\n",
    "\n",
    "print(best_model_tag_metrics.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
